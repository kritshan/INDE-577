{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8bHwTpewcSmRhAdsu8IJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kritshan/INDE-577/blob/main/Supervised%20Learning/Neural%20Networks/feed_forward_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Neural Network\n",
        "\n",
        "In this notebook, we will be implementing a feed forward neural network. Feed forward networks are a type of dense neural network, where information solely flows in one direction through the three layers.\n",
        "\n",
        "Here are the three layers:\n",
        "* Input Layer: This layer contains the input nodes where the data is fed into the network. Each node represents a feature or attribute of the input data.\n",
        "\n",
        "* Hidden Layers: These are intermediary layers between the input and output layers. Each hidden layer consists of multiple nodes (neurons) which apply transformations to the input data through weighted connections.\n",
        "\n",
        "* Output Layer: This layer produces the final output of the network based on the transformations applied to the input data. The number of nodes in the output layer depends on the nature of the problem, such as classification (where each node may represent a class) or regression (where there may be a single node for continuous output).\n",
        "\n",
        "In a feed forward network, there is no cycle of information. Information only goes from the input layer to the hidden layer and finally to the output layer. This is unlike recurrent neural networks where connections allow the information to loop back through the layers. The cycles can create a vanishing gradient problem, as there can be many layers in the network from the repetitive loops. We avoid this problem by using a feed forward network.\n",
        "\n",
        "We will be attempting to classify text documents, in terms of their source. We have three input documents, and we can try to determine which document a sequence of characters belongs to.\n",
        "\n",
        "First, we need to define our data processing steps, the number of classes, batch sizes for gradient descent training, the number of layers, and the number of neurons in each layer.\n",
        "\n",
        "We will implement our network using one of the beginner versions of TensorFlow. This version is more low-level than the newer versions, so we have more visibility into what is occurring.\n"
      ],
      "metadata": {
        "id": "CXUIMsvOfOeu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XIwIf6AnqHEw"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# the number of iterations to train for\n",
        "numTrainingIters = 10000\n",
        "\n",
        "# the number of hidden neurons that hold the state of the RNN\n",
        "hiddenUnits = 500\n",
        "\n",
        "hiddenUnitsLayer1 = 515\n",
        "hiddenUnitsLayer2 = 250\n",
        "\n",
        "# the number of classes that we are learning over\n",
        "numClasses = 3\n",
        "\n",
        "# the number of data points in a batch\n",
        "batchSize = 100\n",
        "\n",
        "# this function takes a dictionary (called data) which contains\n",
        "# of (dataPointID, (classNumber, matrix)) entries.  Each matrix\n",
        "# is a sequence of vectors; each vector has a one-hot-encoding of\n",
        "# an ascii character, and the sequence of vectors corresponds to\n",
        "# one line of text.  classNumber indicates which file the line of\n",
        "# text came from.\n",
        "#\n",
        "# The argument maxSeqLen is the maximum length of a line of text\n",
        "# seen so far.  fileName is the name of a file whose contents\n",
        "# we want to add to data.  classNum is an indicator of the class\n",
        "# we are going to associate with text from that file.  linesToUse\n",
        "# tells us how many lines to sample from the file.\n",
        "#\n",
        "# The return val is the new maxSeqLen, as well as the new data\n",
        "# dictionary with the additional lines of text added\n",
        "def addToData (maxSeqLen, data, testData, fileName, classNum, linesToUse):\n",
        "    #\n",
        "    # open the file and read it in\n",
        "    response = urllib.request.urlopen(fileName)\n",
        "    content = response.readlines ()\n",
        "    #\n",
        "    # sample linesToUse numbers; these will tell us what lines\n",
        "    # from the text file we will use\n",
        "    myInts = np.random.choice(len(content), size=linesToUse + 1000, replace=False)\n",
        "    testInts = myInts[-2000:]\n",
        "    trainingInts = myInts[:10000]\n",
        "    #\n",
        "    # i is the key of the next line of text to add to the dictionary\n",
        "    i = len(data)\n",
        "    #\n",
        "    # loop thru and add the lines of text to the dictionary\n",
        "    for whichLine in trainingInts.flat:\n",
        "        #\n",
        "        # get the line and ignore it if it has nothing in it\n",
        "        line = content[whichLine].decode(\"utf-8\")\n",
        "        if line.isspace () or len(line) == 0:\n",
        "            continue;\n",
        "        #\n",
        "        # take note if this is the longest line we've seen\n",
        "        if len (line) > maxSeqLen:\n",
        "            maxSeqLen = len (line)\n",
        "        #\n",
        "        # create the matrix that will hold this line\n",
        "        temp = np.zeros((len(line), 256))\n",
        "        #\n",
        "        # j is the character we are on\n",
        "        j = 0\n",
        "        #\n",
        "        # loop thru the characters\n",
        "        for ch in line:\n",
        "            #\n",
        "            # non-ascii? ignore\n",
        "            if ord(ch) >= 256:\n",
        "                continue\n",
        "            #\n",
        "            # one hot!\n",
        "            temp[j][ord(ch)] = 1\n",
        "            #\n",
        "            # move onto the next character\n",
        "            j = j + 1\n",
        "            #\n",
        "        # remember the line of text\n",
        "        data[i] = (classNum, temp)\n",
        "        #\n",
        "        # move onto the next line\n",
        "        i = i + 1\n",
        "    #\n",
        "    # and return the dictionary with the new data\n",
        "\n",
        "    testI = len(testData)\n",
        "    for whichLine in testInts.flat:\n",
        "        if len(testData) == 3000:\n",
        "            break\n",
        "        line = content[whichLine].decode(\"utf-8\")\n",
        "        if line.isspace () or len(line) == 0:\n",
        "            continue;\n",
        "        #\n",
        "        # take note if this is the longest line we've seen\n",
        "        if len (line) > maxSeqLen:\n",
        "            maxSeqLen = len (line)\n",
        "        #\n",
        "        # create the matrix that will hold this line\n",
        "        temp = np.zeros((len(line), 256))\n",
        "        #\n",
        "        # j is the character we are on\n",
        "        j = 0\n",
        "        #\n",
        "        # loop thru the characters\n",
        "        for ch in line:\n",
        "            #\n",
        "            # non-ascii? ignore\n",
        "            if ord(ch) >= 256:\n",
        "                continue\n",
        "            #\n",
        "            # one hot!\n",
        "            temp[j][ord(ch)] = 1\n",
        "            #\n",
        "            # move onto the next character\n",
        "            j = j + 1\n",
        "            #\n",
        "        # remember the line of text\n",
        "        testData[testI] = (classNum, temp)\n",
        "        #\n",
        "        # move onto the next line\n",
        "        testI = testI + 1\n",
        "\n",
        "    return (maxSeqLen, data), (maxSeqLen, testData)\n",
        "\n",
        "# this function takes as input a data set encoded as a dictionary\n",
        "# (same encoding as the last function) and pre-pends every line of\n",
        "# text with empty characters so that each line of text is exactly\n",
        "# maxSeqLen characters in size\n",
        "def pad (maxSeqLen, data):\n",
        "   #\n",
        "   # loop thru every line of text\n",
        "   for i in data:\n",
        "        #\n",
        "        # access the matrix and the label\n",
        "        temp = data[i][1]\n",
        "        label = data[i][0]\n",
        "        #\n",
        "        # get the number of chatacters in this line\n",
        "        len = temp.shape[0]\n",
        "        #\n",
        "        # and then pad so the line is the correct length\n",
        "        padding = np.zeros ((maxSeqLen - len,256))\n",
        "        data[i] = (label, np.transpose (np.concatenate ((padding, temp), axis = 0)))\n",
        "   #\n",
        "   # return the new data set\n",
        "   return data\n",
        "\n",
        "# this generates a new batch of training data of size batchSize from the\n",
        "# list of lines of text data. This version of generateData is useful for\n",
        "# an RNN because the data set x is a NumPy array with dimensions\n",
        "# [batchSize, 256, maxSeqLen]; it can be unstacked into a series of\n",
        "# matrices containing one-hot character encodings for each data point\n",
        "# using tf.unstack(inputX, axis=2)\n",
        "def generateDataRNN (maxSeqLen, data):\n",
        "    #\n",
        "    # randomly sample batchSize lines of text\n",
        "    myInts = np.random.randint (0, len(data), batchSize)\n",
        "    #\n",
        "    # stack all of the text into a matrix of one-hot characters\n",
        "    x = np.stack ([data[i][1] for i in myInts.flat])\n",
        "    #\n",
        "    # and stack all of the labels into a vector of labels\n",
        "    y = np.stack ([np.array((data[i][0])) for i in myInts.flat])\n",
        "    #\n",
        "    # return the pair\n",
        "    return (x, y)\n",
        "\n",
        "# this also generates a new batch of training data, but it represents\n",
        "# the data as a NumPy array with dimensions [batchSize, 256 * maxSeqLen]\n",
        "# where for each data point, all characters have been appended.  Useful\n",
        "# for feed-forward network training\n",
        "def generateDataFeedForward (maxSeqLen, data):\n",
        "    #\n",
        "    # randomly sample batchSize lines of text\n",
        "    myInts = np.random.randint (0, len(data), batchSize)\n",
        "    #\n",
        "    # stack all of the text into a matrix of one-hot characters\n",
        "    x = np.stack ([data[i][1].flatten () for i in myInts.flat])\n",
        "    #\n",
        "    # and stack all of the labels into a vector of labels\n",
        "    y = np.stack ([np.array((data[i][0])) for i in myInts.flat])\n",
        "    #\n",
        "    # return the pair\n",
        "    return (x, y)\n",
        "\n",
        "# create the data dictionary\n",
        "maxSeqLen = 0\n",
        "data = {}\n",
        "testData = {}\n",
        "\n",
        "# load up the three data sets and the test data sets\n",
        "(maxSeqLen, data), (maxSeqLen, testData) = addToData (maxSeqLen, data, testData, \"https://s3.amazonaws.com/chrisjermainebucket/text/Holmes.txt\", 0, 11000)\n",
        "(maxSeqLen, data), (maxSeqLen, testData) = addToData (maxSeqLen, data, testData, \"https://s3.amazonaws.com/chrisjermainebucket/text/war.txt\", 1, 11000)\n",
        "(maxSeqLen, data), (maxSeqLen, testData) = addToData (maxSeqLen, data, testData, \"https://s3.amazonaws.com/chrisjermainebucket/text/william.txt\", 2, 11000)\n",
        "\n",
        "# pad each entry in the dictionary with empty characters as needed so\n",
        "# that the sequences are all of the same length\n",
        "data = pad (maxSeqLen, data)\n",
        "testData = pad (maxSeqLen, testData)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have processed our data and prepared our training/test datasets, we are ready to define our TensorFlow variables and placeholders. Variables are tensors/arrays whose values will be learned. Placeholders are values that are given at training times.\n",
        "\n",
        "After that, we are prepared to implement our training process and our testing session. We will utilize a cross entropy loss function and an Adaptive Gradient Descent Algorithm for training.\n",
        "\n",
        "For testing, we will see how many documents the network was able to correctly predict our of 3000 test documents."
      ],
      "metadata": {
        "id": "U6IpSxNskQ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we build the TensorFlow computation... there are two inputs,\n",
        "# a batch of text lines and a batch of labels\n",
        "inputX = tf.placeholder(tf.float32, [batchSize, 256 * maxSeqLen])\n",
        "inputY = tf.placeholder(tf.int32, [batchSize])\n",
        "\n",
        "# this is the inital state of the RNN, before processing any data\n",
        "initialState = tf.placeholder(tf.float32, [batchSize, hiddenUnits])\n",
        "\n",
        "# Define weights and biases for the hidden layers and output layer\n",
        "W1 = tf.Variable(np.random.normal(0, 0.01, (256 * maxSeqLen, hiddenUnitsLayer1)), dtype=tf.float32)\n",
        "b1 = tf.Variable(np.zeros((1, hiddenUnitsLayer1)), dtype=tf.float32)\n",
        "\n",
        "W2 = tf.Variable(np.random.normal(0, 0.01, (hiddenUnitsLayer1, hiddenUnitsLayer2)), dtype=tf.float32)\n",
        "b2 = tf.Variable(np.zeros((1, hiddenUnitsLayer2)), dtype=tf.float32)\n",
        "\n",
        "W3 = tf.Variable(np.random.normal(0, 0.01, (hiddenUnitsLayer2, numClasses)), dtype=tf.float32)\n",
        "b3 = tf.Variable(np.zeros((1, numClasses)), dtype=tf.float32)\n",
        "\n",
        "\n",
        "# Define the network architecture\n",
        "hiddenLayer1 = tf.nn.relu(tf.matmul(inputX, W1) + b1)\n",
        "hiddenLayer2 = tf.nn.relu(tf.matmul(hiddenLayer1, W2) + b2)\n",
        "\n",
        "\n",
        "# compute the set of outputs\n",
        "outputs = tf.matmul(hiddenLayer2, W3) + b3\n",
        "\n",
        "predictions = tf.nn.softmax(outputs)\n",
        "\n",
        "# compute the loss\n",
        "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=outputs, labels=inputY)\n",
        "totalLoss = tf.reduce_mean(losses)\n",
        "\n",
        "# use gradient descent to train\n",
        "trainingAlg = tf.compat.v1.train.AdagradOptimizer(0.01).minimize(totalLoss)\n",
        "\n",
        "# and train!!\n",
        "with tf.Session() as sess:\n",
        "    #\n",
        "    # initialize everything\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    #\n",
        "    # and run the training iters\n",
        "    for epoch in range(numTrainingIters):\n",
        "        #\n",
        "        # get some data\n",
        "        x, y = generateDataFeedForward(maxSeqLen, data)\n",
        "        #\n",
        "        # do the training epoch\n",
        "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
        "        _totalLoss, _trainingAlg, _predictions, _outputs = sess.run(\n",
        "                [totalLoss, trainingAlg, predictions, outputs],\n",
        "                feed_dict={\n",
        "                    inputX:x,\n",
        "                    inputY:y,\n",
        "                })\n",
        "        #\n",
        "        # just FYI, compute the number of correct predictions\n",
        "        numCorrect = 0\n",
        "        for i in range (len(y)):\n",
        "           maxPos = -1\n",
        "           maxVal = 0.0\n",
        "           for j in range (numClasses):\n",
        "               if maxVal < _predictions[i][j]:\n",
        "                   maxVal = _predictions[i][j]\n",
        "                   maxPos = j\n",
        "           if maxPos == y[i]:\n",
        "               numCorrect = numCorrect + 1\n",
        "        #\n",
        "        # print out to the screen only the first 30 and last 30 loss values\n",
        "        if epoch < 30 or epoch > 9970:\n",
        "            print(\"Step\", epoch, \"Loss\", _totalLoss, \"Correct\", numCorrect, \"out of\", batchSize)\n",
        "\n",
        "    testX = np.stack([data[i][1].flatten() for i in testData])\n",
        "    testY = np.stack([data[i][0] for i in testData])\n",
        "    numCorrect = 0\n",
        "    losses = []\n",
        "    for i in range(30):\n",
        "        x = testX[(i*batchSize):((i+1) * batchSize)]\n",
        "        y = testY[(i*batchSize):((i+1) * batchSize)]\n",
        "        #\n",
        "        # do the training epoch\n",
        "        _currentState = np.zeros((batchSize, hiddenUnits))\n",
        "        _totalLoss, _predictions = sess.run(\n",
        "                [totalLoss, predictions],\n",
        "                feed_dict={\n",
        "                    inputX:x,\n",
        "                    inputY:y,\n",
        "                })\n",
        "        losses.append(_totalLoss)\n",
        "        #\n",
        "        # just FYI, compute the number of correct predictions\n",
        "        for i in range (len(y)):\n",
        "           maxPos = -1\n",
        "           maxVal = 0.0\n",
        "           for j in range (numClasses):\n",
        "               if maxVal < _predictions[i][j]:\n",
        "                   maxVal = _predictions[i][j]\n",
        "                   maxPos = j\n",
        "           if maxPos == y[i]:\n",
        "               numCorrect = numCorrect + 1\n",
        "    totalLosses = np.mean(losses)\n",
        "    print(\"Loss for 3000 randomly chosen documents is\", totalLosses, \"correct labels is\", numCorrect, \"out of 3000\")\n"
      ],
      "metadata": {
        "id": "Hv65MIxFjej6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153d155c-b871-4f98-e060-a3dc6a52f6cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 Loss 1.098646 Correct 27 out of 100\n",
            "Step 1 Loss 1.0983856 Correct 39 out of 100\n",
            "Step 2 Loss 1.0981177 Correct 43 out of 100\n",
            "Step 3 Loss 1.096858 Correct 49 out of 100\n",
            "Step 4 Loss 1.0976019 Correct 40 out of 100\n",
            "Step 5 Loss 1.0973445 Correct 40 out of 100\n",
            "Step 6 Loss 1.0976609 Correct 37 out of 100\n",
            "Step 7 Loss 1.0975741 Correct 38 out of 100\n",
            "Step 8 Loss 1.100269 Correct 27 out of 100\n",
            "Step 9 Loss 1.0992637 Correct 31 out of 100\n",
            "Step 10 Loss 1.0982443 Correct 35 out of 100\n",
            "Step 11 Loss 1.0997188 Correct 29 out of 100\n",
            "Step 12 Loss 1.0980484 Correct 36 out of 100\n",
            "Step 13 Loss 1.0974232 Correct 39 out of 100\n",
            "Step 14 Loss 1.098426 Correct 34 out of 100\n",
            "Step 15 Loss 1.0982517 Correct 35 out of 100\n",
            "Step 16 Loss 1.0970558 Correct 39 out of 100\n",
            "Step 17 Loss 1.0970098 Correct 42 out of 100\n",
            "Step 18 Loss 1.098768 Correct 34 out of 100\n",
            "Step 19 Loss 1.0967114 Correct 41 out of 100\n",
            "Step 20 Loss 1.096355 Correct 41 out of 100\n",
            "Step 21 Loss 1.0981934 Correct 35 out of 100\n",
            "Step 22 Loss 1.0957505 Correct 43 out of 100\n",
            "Step 23 Loss 1.0939204 Correct 46 out of 100\n",
            "Step 24 Loss 1.0954003 Correct 41 out of 100\n",
            "Step 25 Loss 1.1009374 Correct 29 out of 100\n",
            "Step 26 Loss 1.1002505 Correct 30 out of 100\n",
            "Step 27 Loss 1.0958734 Correct 41 out of 100\n",
            "Step 28 Loss 1.097335 Correct 37 out of 100\n",
            "Step 29 Loss 1.0940415 Correct 44 out of 100\n",
            "Step 9971 Loss 0.020838432 Correct 100 out of 100\n",
            "Step 9972 Loss 0.050447725 Correct 98 out of 100\n",
            "Step 9973 Loss 0.053581573 Correct 99 out of 100\n",
            "Step 9974 Loss 0.077633254 Correct 97 out of 100\n",
            "Step 9975 Loss 0.02302284 Correct 100 out of 100\n",
            "Step 9976 Loss 0.037881847 Correct 99 out of 100\n",
            "Step 9977 Loss 0.05874828 Correct 98 out of 100\n",
            "Step 9978 Loss 0.03632866 Correct 100 out of 100\n",
            "Step 9979 Loss 0.062868595 Correct 99 out of 100\n",
            "Step 9980 Loss 0.11356328 Correct 96 out of 100\n",
            "Step 9981 Loss 0.032499664 Correct 100 out of 100\n",
            "Step 9982 Loss 0.04622866 Correct 100 out of 100\n",
            "Step 9983 Loss 0.060633853 Correct 97 out of 100\n",
            "Step 9984 Loss 0.053022895 Correct 100 out of 100\n",
            "Step 9985 Loss 0.053921804 Correct 98 out of 100\n",
            "Step 9986 Loss 0.05782689 Correct 98 out of 100\n",
            "Step 9987 Loss 0.02905672 Correct 100 out of 100\n",
            "Step 9988 Loss 0.030684192 Correct 100 out of 100\n",
            "Step 9989 Loss 0.039027896 Correct 99 out of 100\n",
            "Step 9990 Loss 0.032196112 Correct 100 out of 100\n",
            "Step 9991 Loss 0.04314274 Correct 98 out of 100\n",
            "Step 9992 Loss 0.057411294 Correct 99 out of 100\n",
            "Step 9993 Loss 0.030016022 Correct 100 out of 100\n",
            "Step 9994 Loss 0.044841748 Correct 100 out of 100\n",
            "Step 9995 Loss 0.04364012 Correct 99 out of 100\n",
            "Step 9996 Loss 0.080749296 Correct 98 out of 100\n",
            "Step 9997 Loss 0.057619568 Correct 99 out of 100\n",
            "Step 9998 Loss 0.068368785 Correct 99 out of 100\n",
            "Step 9999 Loss 0.046348885 Correct 99 out of 100\n",
            "Loss for 3000 randomly chosen documents is 0.09125849 correct labels is 2917 out of 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "Great! We were successfuly able to predict 2917 documents out of the 3000 test documents. It is evident that feed forward neural networks are powerful models, and they tools like TensorFlow make them relatively easy to understand. Unlike some of the other models in our exploration, they are black-box models. Their inner processes are not easily interpretable.\n",
        "\n",
        "Nonetheless, feed forward neural networks are widely used in various applications, including image and speech recognition, natural language processing, financial forecasting, and more. They serve as a fundamental building block for more complex neural network architectures like convolutional neural networks and recurrent neural networks."
      ],
      "metadata": {
        "id": "FyN2iA4dlmbt"
      }
    }
  ]
}